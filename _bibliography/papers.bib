---
---

@article{DBLP:journals/corr/abs-1901-10788,
  author    = {Gal Katzhendler, Daphna Weinshall},
  title     = {Blurred Images Lead to Bad Local Minima},
  journal   = {CoRR},
  volume    = {abs/1901.10788},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.10788},
  eprinttype = {arXiv},
  arxiv    = {1901.10788},
  timestamp = {Sun, 03 Feb 2019 14:23:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-10788.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {High Initial visual Acuity (HIA) in newborns treated for
cataracts, it is argued by Vogelsang et al. (2018), may cause
impairments in configural face analysis. This HIA hypothesis
is contrary to the standard explanation by a critical period for
learning face processing. The hypothesis is supported by com-
putational experiments with an artificial neural network. In
our work we argue that the computational methodology used to
evaluate the HIA hypothesis is flawed. It essentially shows that
when a classifier is tested with images from different resolu-
tions, the classifier benefits from seeing images from different
resolution during its training. We therefore offer a better-fitting
methodology; employing the modified methodology - the HIA
hypothesis does not hold in simulations using the same artifi-
cial neural network model, and the same data. Vogelsang et al.
(2018) also show that initial exposure to low resolution images
gives rise to larger receptive fields. Our last set of experiments
tests the hypothesis that this might be the underlying reason
for the observed impairments. Once again, we are unable to
find an advantage to training with images of low initial acuity.
We therefore conclude that simulations with artificial networks
do not support the hypothesis that High Initial visual Acuity is
detrimental.},
  url = {https://arxiv.org/abs/1901.10788},
  pdf = {https://arxiv.org/pdf/1901.10788.pdf}
}

@article{doi:10.1073/pnas.1906400116,
  author = {Gal Katzhendler, Daphna Weinshall },
  title = {Potential upside of high initial visual acuity?},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {38},
  pages = {18765-18766},
  year = {2019},
  doi = {10.1073/pnas.1906400116},
  URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1906400116},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1906400116},
  abstract = {Vogelsang et al. (1) argue that high initial visual acuity in children, who underwent late treatment of congenital blindness, may be responsible for subsequent impairments in configural face analysis. This hypothesis offers an exciting alternative to the standard explanation of a critical period for the ensuing impairment, which could have dramatic implications on the follow-up treatment of such children. However, close inspection of the supporting computational argument provided in ref. 1 casts doubt on this conclusion. After broadening the analysis, we conclude that the computational study in ref. 1 cannot be used as evidence that initial exposure to low-resolution images is necessarily beneficial. We were also unable to confirm another proposed implication, that initial presentation of low-resolution images can improve the generalization performance of artificial neural networks in general, by promoting the development of larger receptive fields (2).},
  pdf = {https://www.pnas.org/doi/epdf/10.1073/pnas.1906400116},
  url = {https://www.pnas.org/doi/10.1073/pnas.1906400116},
  selected = {true}

}

@misc{daniely2022approximate,
      title={Approximate Description Length, Covering Numbers, and VC Dimension}, 
      author={Amit Daniely, Gal Katzhendler},
      year={2022},
      arxiv={2209.12882},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      abstract={Recently, Daniely and Granot introduced a new notion of complexity called Approximate Description Length (ADL). They used it to derive novel generalization bounds for neural networks, that despite substantial work, were out of reach for more classical techniques such as discretization, Covering Numbers and Rademacher Complexity. In this paper we explore how ADL relates to classical notions of function complexity such as Covering Numbers and VC Dimension. We find that for functions whose range is the reals, ADL is essentially equivalent to these classical complexity measures. However, this equivalence breaks for functions with high dimensional range.},
      pdf={https://arxiv.org/pdf/2209.12882.pdf},
      selected={true}
}
